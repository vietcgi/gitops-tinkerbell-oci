#cloud-config
#
# Cloud-init configuration for Metal Foundry control plane
# This prepares the system and optionally runs the cluster setup
#

hostname: ${hostname}

# Update and install basic packages
package_update: true
package_upgrade: true

# Set root password using hash
chpasswd:
  list: |
    root:$6$6cUHnjJ1JvrwzVVL$9qXzyIVh1CgCIaLkSyadZ2gJeiofWZ0rAVUoPWQQPMMFW31Y2dmXToe3jzG3wZPnXh3gvv3EGZGxXh/mcvS.S1
  expire: False
ssh_pwauth: True

packages:
  - curl
  - wget
  - git
  - jq
  - unzip
  - apt-transport-https
  - ca-certificates
  - gnupg
  - lsb-release
  - iptables
  - iptables-persistent
  - open-iscsi
  - nfs-common
  - chrony
  - socat

# Configure system settings
write_files:
  # Metal Foundry comprehensive sysctl settings (from setup-cluster.sh)
  - path: /etc/sysctl.d/99-metal-foundry.conf
    content: |
      # Metal Foundry - Production System Tuning
      # Based on ansible-infra common role

      # Network performance - Optimized for high throughput
      net.ipv4.ip_forward = 1
      net.core.somaxconn = 65536
      net.ipv4.tcp_max_syn_backlog = 2048
      net.ipv4.tcp_tw_reuse = 1
      net.ipv4.tcp_fin_timeout = 10

      # Multipath routing optimizations
      net.ipv4.fib_multipath_hash_policy = 1
      net.ipv4.fib_multipath_use_neigh = 1

      # ARP cache optimizations (high-traffic environments)
      net.ipv4.neigh.default.gc_thresh1 = 80000
      net.ipv4.neigh.default.gc_thresh2 = 90000
      net.ipv4.neigh.default.gc_thresh3 = 100000

      # TCP buffer optimization (throughput for gigabit+ networks)
      net.ipv4.tcp_rmem = 4096 131072 6291456
      net.ipv4.tcp_wmem = 4096 16384 4194304

      # Port range for high-connection scenarios
      net.ipv4.ip_local_port_range = 1024 65535
      net.ipv4.tcp_moderate_rcvbuf = 1

      # File system optimizations
      fs.file-max = 12000500
      fs.nr_open = 20000500

      # Security hardening
      net.ipv4.conf.all.send_redirects = 0
      net.ipv4.conf.default.send_redirects = 0
      net.ipv4.conf.all.accept_source_route = 0
      net.ipv4.conf.default.accept_source_route = 0

      # ICMP hardening
      net.ipv4.icmp_echo_ignore_broadcasts = 1
      net.ipv4.icmp_ignore_bogus_error_responses = 1

      # Kubernetes/Cilium specific
      net.bridge.bridge-nf-call-iptables = 1
      net.bridge.bridge-nf-call-ip6tables = 1

      # Memory overcommit for Kubernetes
      vm.overcommit_memory = 1
      vm.panic_on_oom = 0
      vm.swappiness = 0

      # Inotify limits for large clusters
      fs.inotify.max_user_watches = 524288
      fs.inotify.max_user_instances = 512

  # Metal Foundry file limits
  - path: /etc/security/limits.d/99-metal-foundry.conf
    content: |
      # Metal Foundry - Production File Limits
      # Based on ansible-infra common role

      # File descriptor limits
      * soft nofile 1039999
      * hard nofile 1039999
      root soft nofile 1039999
      root hard nofile 1039999

      # Process limits for high-concurrency environments
      * soft nproc 9999999
      * hard nproc 9999999
      root soft nproc unlimited
      root hard nproc unlimited

      # Memory lock (for Kubernetes)
      * soft memlock unlimited
      * hard memlock unlimited

  # Systemd limits
  - path: /etc/systemd/system.conf.d/99-metal-foundry.conf
    content: |
      [Manager]
      DefaultLimitNOFILE=1039999
      DefaultLimitNPROC=9999999
      DefaultLimitMEMLOCK=infinity

  # Chrony NTP configuration
  - path: /etc/chrony/chrony.conf
    content: |
      # Metal Foundry - Chrony NTP Configuration
      # Use Oracle Cloud NTP servers (OCI instances)
      server 169.254.169.123 prefer iburst minpoll 4 maxpoll 4

      # Fallback to public NTP pools
      pool time.google.com iburst maxsources 4
      pool time.cloudflare.com iburst maxsources 2
      pool pool.ntp.org iburst maxsources 4

      # Record the rate at which the system clock gains/loses time
      driftfile /var/lib/chrony/drift

      # Allow the system clock to be stepped in the first three updates
      makestep 1.0 3

      # Enable kernel synchronization of the real-time clock (RTC)
      rtcsync

      # Specify directory for log files
      logdir /var/log/chrony

      # Enable hardware timestamping if available
      hwtimestamp *

  # Tinkerbell gRPC proxy service (socat forwards external 42113 to Tinkerbell ClusterIP)
  # This enables tink-worker on bare metal nodes to reach the Tinkerbell gRPC server
  - path: /etc/systemd/system/tinkerbell-grpc-proxy.service
    content: |
      [Unit]
      Description=Tinkerbell gRPC Proxy (socat)
      Documentation=https://tinkerbell.org
      After=network.target k3s.service
      Wants=k3s.service

      [Service]
      Type=simple
      # Get Tinkerbell service ClusterIP dynamically, fallback to known IP
      ExecStartPre=/bin/bash -c 'until kubectl get svc tinkerbell -n tink-system -o jsonpath="{.spec.clusterIP}" 2>/dev/null | grep -q "^10\."; do sleep 5; done'
      ExecStart=/bin/bash -c 'TINK_IP=$(kubectl get svc tinkerbell -n tink-system -o jsonpath="{.spec.clusterIP}" 2>/dev/null || echo "10.43.144.5"); exec /usr/bin/socat TCP-LISTEN:42113,fork,reuseaddr TCP:$${TINK_IP}:42113'
      Restart=always
      RestartSec=5

      [Install]
      WantedBy=multi-user.target

  # Smee HTTP proxy service (socat forwards external 7171 to Tinkerbell ClusterIP)
  # This enables iPXE clients to fetch auto.ipxe scripts from remote Smee
  - path: /etc/systemd/system/smee-http-proxy.service
    content: |
      [Unit]
      Description=Smee HTTP Proxy (socat)
      Documentation=https://tinkerbell.org
      After=network.target k3s.service
      Wants=k3s.service

      [Service]
      Type=simple
      # Get Tinkerbell service ClusterIP dynamically
      ExecStartPre=/bin/bash -c 'until kubectl get svc tinkerbell -n tink-system -o jsonpath="{.spec.clusterIP}" 2>/dev/null | grep -q "^10\."; do sleep 5; done'
      ExecStart=/bin/bash -c 'TINK_IP=$(kubectl get svc tinkerbell -n tink-system -o jsonpath="{.spec.clusterIP}" 2>/dev/null || echo "10.43.144.5"); exec /usr/bin/socat TCP-LISTEN:7171,fork,reuseaddr TCP:$${TINK_IP}:7171'
      Restart=always
      RestartSec=5

      [Install]
      WantedBy=multi-user.target

  # K3s configuration (with kube-proxy disabled for Cilium)
  - path: /etc/rancher/k3s/config.yaml
    content: |
      disable:
        - flannel
        - traefik
        - servicelb
        - local-storage
        - kube-proxy
      flannel-backend: none
      disable-network-policy: true
      disable-kube-proxy: true
      write-kubeconfig-mode: "0644"
      tls-san:
        - "${hostname}"
        - "localhost"
        - "127.0.0.1"

  # Setup script with retry logic
  - path: /usr/local/bin/metal-foundry-setup
    permissions: '0755'
    content: |
      #!/bin/bash
      set -euo pipefail

      SCRIPT_URL="https://raw.githubusercontent.com/vietcgi/gitops-tinkerbell-oci/main/scripts/setup-cluster.sh"
      LOG_FILE="/var/log/metal-foundry-setup.log"
      MAX_RETRIES=5
      RETRY_DELAY=30

      log() { echo "[$(date '+%Y-%m-%d %H:%M:%S')] $*" | tee -a "$LOG_FILE"; }

      download_with_retry() {
        local url=$1
        local output=$2
        local attempt=1

        while [ $attempt -le $MAX_RETRIES ]; do
          log "Downloading setup script (attempt $attempt/$MAX_RETRIES)..."
          if curl -sfL "$url" -o "$output" && [ -s "$output" ]; then
            log "Download successful"
            return 0
          fi
          log "Download failed, waiting $${RETRY_DELAY}s before retry..."
          sleep $RETRY_DELAY
          attempt=$((attempt + 1))
        done

        log "ERROR: Failed to download after $MAX_RETRIES attempts"
        return 1
      }

      main() {
        log "=== Metal Foundry Setup Starting ==="

        # Download setup script
        if ! download_with_retry "$SCRIPT_URL" "/tmp/setup-cluster.sh"; then
          log "ERROR: Could not download setup script"
          log "Manual setup required. SSH in and run:"
          log "  curl -sfL $SCRIPT_URL | sudo bash"
          exit 1
        fi

        chmod +x /tmp/setup-cluster.sh

        # Run setup
        log "Running setup script..."
        if /tmp/setup-cluster.sh; then
          log "=== Setup Complete ==="
          echo "complete" > /var/log/metal-foundry-status
        else
          log "ERROR: Setup script failed"
          echo "failed" > /var/log/metal-foundry-status
          exit 1
        fi
      }

      main "$@"

runcmd:
  # Disable swap
  - swapoff -a
  - sed -i '/swap/d' /etc/fstab

  # Load kernel modules
  - modprobe br_netfilter
  - modprobe overlay
  - echo "br_netfilter" >> /etc/modules-load.d/k8s.conf
  - echo "overlay" >> /etc/modules-load.d/k8s.conf

  # Reload systemd with new limits
  - systemctl daemon-reexec

  # Apply sysctl settings
  - sysctl --system

  # Configure and start chrony
  - systemctl enable chrony
  - systemctl restart chrony

  # Configure iptables
  # IMPORTANT: Use -I (insert) instead of -A (append) to add rules BEFORE
  # OCI default REJECT rule. Rules inserted in reverse order (last -I is first rule).
  - iptables -I INPUT 1 -i lo -j ACCEPT
  - iptables -I INPUT 2 -m state --state ESTABLISHED,RELATED -j ACCEPT
  - iptables -I INPUT 3 -s 10.0.0.0/8 -j ACCEPT
  - iptables -I INPUT 4 -s 10.42.0.0/16 -j ACCEPT
  - iptables -I INPUT 5 -s 10.43.0.0/16 -j ACCEPT
  - iptables -I INPUT 6 -p tcp --dport 22 -j ACCEPT
  - iptables -I INPUT 7 -p tcp --dport 80 -j ACCEPT
  - iptables -I INPUT 8 -p tcp --dport 443 -j ACCEPT
  - iptables -I INPUT 9 -p tcp --dport 6443 -j ACCEPT
  - iptables -I INPUT 10 -p tcp --dport 8080 -j ACCEPT
  - iptables -I INPUT 11 -p tcp --dport 10250 -j ACCEPT
  - iptables -I INPUT 12 -p udp --dport 41641 -j ACCEPT
  - iptables -I INPUT 13 -p icmp -j ACCEPT
  # Tinkerbell ports - locked to colo /27 subnet
  - iptables -I INPUT 14 -s 108.181.38.64/27 -p tcp --dport 42113 -j ACCEPT
  - iptables -I INPUT 15 -s 108.181.38.64/27 -p tcp --dport 7171 -j ACCEPT
  - iptables -A FORWARD -j ACCEPT
  - netfilter-persistent save 2>/dev/null || true

  # Start iscsid for storage
  - systemctl enable iscsid
  - systemctl start iscsid

%{ if tailscale_auth_key != "" }
  # Install Tailscale
  - curl -fsSL https://tailscale.com/install.sh | sh
  - tailscale up --auth-key="${tailscale_auth_key}" --hostname="${hostname}" --accept-routes
%{ endif }

  # Run cluster setup
  - /usr/local/bin/metal-foundry-setup

  # Enable Tinkerbell proxies (after k3s is running)
  - systemctl daemon-reload
  - systemctl enable tinkerbell-grpc-proxy.service
  - systemctl enable smee-http-proxy.service
  - systemctl start tinkerbell-grpc-proxy.service || true
  - systemctl start smee-http-proxy.service || true

final_message: |
  Metal Foundry cloud-init complete. Elapsed time: $UPTIME seconds.

  Check setup status:
    cat /var/log/metal-foundry-status

  View setup logs:
    sudo tail -f /var/log/metal-foundry-setup.log

  If setup failed, run manually:
    sudo /usr/local/bin/metal-foundry-setup
